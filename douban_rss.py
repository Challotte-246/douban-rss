import requests
from bs4 import BeautifulSoup
import datetime
import time
import re
import os
import random
import xml.etree.ElementTree as ET
from urllib.parse import urljoin

# ===== é…ç½®åŒºåŸŸ =====
GROUP_ID = "713925"  # è±†ç“£å°ç»„ID
RETRY_COUNT = 3      # è¯·æ±‚å¤±è´¥é‡è¯•æ¬¡æ•°
DEBUG_MODE = True    # å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º

# ç”¨æˆ·ä»£ç†åˆ—è¡¨ï¼ˆéšæœºé€‰æ‹©é˜²æ­¢å°ç¦ï¼‰
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0",
]

# ===== è¾…åŠ©å‡½æ•° =====
def get_random_user_agent():
    return random.choice(USER_AGENTS)

def safe_request(url, headers, retry=RETRY_COUNT):
    for attempt in range(retry):
        try:
            if DEBUG_MODE:
                print(f"ğŸ“¡ å°è¯•è¯·æ±‚: {url} (ç¬¬ {attempt+1}/{retry} æ¬¡å°è¯•)")
            delay = random.uniform(1.0, 3.0)
            time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=15)
            
            if DEBUG_MODE:
                print(f"ğŸ”„ HTTP çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                if DEBUG_MODE:
                    print("âœ… è¯·æ±‚æˆåŠŸ!")
                return response
                
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥: HTTP {response.status_code}, ç¬¬ {attempt+1} æ¬¡é‡è¯•...")
            
        except requests.exceptions.Timeout:
            print(f"â±ï¸ è¯·æ±‚è¶…æ—¶, ç¬¬ {attempt+1} æ¬¡é‡è¯•...")
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}, ç¬¬ {attempt+1} æ¬¡é‡è¯•...")
        
        sleep_time = 2 ** (attempt + 1)
        print(f"ğŸ’¤ ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
        time.sleep(sleep_time)
    
    raise Exception(f"ğŸš« å¤šæ¬¡è¯·æ±‚å¤±è´¥: {url}")

def parse_douban_time(time_str):
    now = datetime.datetime.now()
    
    if DEBUG_MODE:
        print(f"â° åŸå§‹æ—¶é—´å­—ç¬¦ä¸²: '{time_str}'")
    
    try:
        if ':' in time_str and len(time_str) <= 5:
            parts = time_str.split(':')
            hour = int(parts[0])
            minute = int(parts[1]) if len(parts) > 1 else 0
            return datetime.datetime(now.year, now.month, now.day, hour, minute)
        
        elif re.match(r'^\d{1,2}-\d{1,2}$', time_str):
            parts = time_str.split('-')
            month = int(parts[0])
            day = int(parts[1])
            return datetime.datetime(now.year, month, day)
        
        elif re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', time_str):
            return datetime.datetime.strptime(time_str, "%Y-%m-%d")
        
        elif "æ˜¨å¤©" in time_str:
            time_part = time_str.replace("æ˜¨å¤©", "").strip()
            if ':' in time_part:
                parts = time_part.split(':')
                hour = int(parts[0])
                minute = int(parts[1]) if len(parts) > 1 else 0
            else:
                hour, minute = 0, 0
            yesterday = now - datetime.timedelta(days=1)
            return datetime.datetime(yesterday.year, yesterday.month, yesterday.day, hour, minute)
            
    except Exception as e:
        print(f"âŒ æ—¶é—´è§£æé”™è¯¯: {str(e)}")
    
    print(f"âš ï¸ æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: '{time_str}'ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä»£æ›¿")
    return now

# ===== ä¸»æŠ“å–å‡½æ•° =====
def fetch_discussion_posts():
    """æŠ“å–è®¨è®ºåŒºå¸–å­ï¼ˆåŒ…å«æœ€æ–°å’Œæœ€çƒ­ï¼‰"""
    print(f"\nğŸ” å¼€å§‹æŠ“å–å°ç»„ {GROUP_ID} çš„è®¨è®ºé¡µé¢...")
    
    url = f"https://www.douban.com/group/{GROUP_ID}/discussion"
    user_agent = get_random_user_agent()
    headers = {
        "User-Agent": user_agent,
        "Referer": f"https://www.douban.com/group/{GROUP_ID}/",
    }
    
    print(f"ğŸŒ ç›®æ ‡URL: {url}")
    print(f"ğŸ§ª ä½¿ç”¨User-Agent: {user_agent}")
    
    try:
        response = safe_request(url, headers)
        response.encoding = 'utf-8'
    except Exception as e:
        print(f"ğŸ”¥ æŠ“å–å¤±è´¥: {str(e)}")
        return []
        
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æŸ¥æ‰¾å¸–å­å®¹å™¨
    post_table = soup.select_one('table.olt')
    if not post_table:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°å¸–å­è¡¨æ ¼")
        return []
    
    posts = []
    for row in post_table.select('tr')[1:]:
        title_cell = row.select_one('td.title')
        if not title_cell: continue
            
        title_link = title_cell.find('a')
        if not title_link: continue
            
        # æå–æ ‡é¢˜å’Œé“¾æ¥
        title = title_link.get_text(strip=True)
        link = title_link.get('href', '')
        
        # æå–å›å¤æ•°
        reply_cell = row.select_one('td.r-count')
        reply_count = int(reply_cell.get_text(strip=True)) if reply_cell else 0
        
        # æå–æ—¶é—´
        time_cell = row.select_one('td.time')
        time_str = time_cell.get_text(strip=True) if time_cell else "æœªçŸ¥æ—¶é—´"
        
        try:
            post_time = parse_douban_time(time_str)
        except:
            post_time = datetime.datetime.now()
            
        # æå–åˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚[è®¨è®º]ã€[è½¬è®©]ç­‰ï¼‰
        category = ""
        if title.startswith('[') and ']' in title:
            end_index = title.find(']') + 1
            category = title[:end_index]
            title = title[end_index:].strip()
        
        posts.append({
            "title": title,
            "link": link,
            "pubDate": post_time,
            "raw_time": time_str,
            "reply_count": reply_count,
            "category": category
        })
    
    print(f"âœ… æˆåŠŸæŠ“å– {len(posts)} æ¡è®¨è®ºåŒºå¸–å­")
    
    # è¿”å›åŸå§‹å¸–å­åˆ—è¡¨ï¼ˆç”¨äºåç»­æ’åºï¼‰
    return posts

def fetch_elite_posts():
    """æŠ“å–ç²¾ååŒºå¸–å­"""
    print(f"\nğŸ” å¼€å§‹æŠ“å–å°ç»„ {GROUP_ID} çš„ç²¾ååŒº...")
    
    url = f"https://www.douban.com/group/{GROUP_ID}/discussion?type=elite"
    user_agent = get_random_user_agent()
    headers = {
        "User-Agent": user_agent,
        "Referer": f"https://www.douban.com/group/{GROUP_ID}/",
    }
    
    print(f"ğŸŒ ç›®æ ‡URL: {url}")
    print(f"ğŸ§ª ä½¿ç”¨User-Agent: {user_agent}")
    
    try:
        response = safe_request(url, headers)
        response.encoding = 'utf-8'
    except Exception as e:
        print(f"ğŸ”¥ æŠ“å–å¤±è´¥: {str(e)}")
        return []
        
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æŸ¥æ‰¾ç²¾ååŒºå¸–å­å®¹å™¨
    elite_posts = []
    topic_items = soup.select('div.topic-item')
    
    if not topic_items:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ç²¾ååŒºå¸–å­")
        return []
    
    for item in topic_items:
        # æå–æ ‡é¢˜å’Œé“¾æ¥
        title_link = item.select_one('div.title a')
        if not title_link: continue
            
        title = title_link.get_text(strip=True)
        link = title_link.get('href', '')
        
        # æå–ä½œè€…
        author_elem = item.select_one('span.author a')
        author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
        
        # æå–å›å¤æ•°å’Œæ”¶è—æ•°
        stats = item.select_one('span.stats')
        reply_count = 0
        if stats:
            # æå–æ ¼å¼å¦‚ï¼š"12å›å¤ 45æ”¶è—"
            match = re.search(r'(\d+)å›å¤', stats.get_text())
            if match:
                reply_count = int(match.group(1))
        
        # æå–æ—¶é—´
        time_elem = item.select_one('span.time')
        time_str = time_elem.get_text(strip=True) if time_elem else "æœªçŸ¥æ—¶é—´"
        
        try:
            post_time = parse_douban_time(time_str)
        except:
            post_time = datetime.datetime.now()
            
        # æå–åˆ†ç±»æ ‡ç­¾
        category = ""
        if title.startswith('[') and ']' in title:
            end_index = title.find(']') + 1
            category = title[:end_index]
            title = title[end_index:].strip()
        
        elite_posts.append({
            "title": title,
            "link": link,
            "pubDate": post_time,
            "raw_time": time_str,
            "reply_count": reply_count,
            "author": author,
            "category": category,
            "is_elite": True  # æ ‡è®°ä¸ºç²¾åå¸–
        })
    
    print(f"âœ… æˆåŠŸæŠ“å– {len(elite_posts)} æ¡ç²¾åå¸–")
    return elite_posts

# ===== RSSç”Ÿæˆå‡½æ•° =====
def generate_rss(posts, feed_type="new"):
    """ç”ŸæˆRSS XML
    
    feed_type: 
        'new' - æœ€æ–°å¸–å­
        'hot' - çƒ­é—¨å¸–å­
        'elite' - ç²¾åå¸–å­
    """
    # è®¾ç½®æ ‡é¢˜å’Œæè¿°
    titles = {
        "new": f"è±†ç“£å°ç»„ {GROUP_ID} æœ€æ–°æ›´æ–°",
        "hot": f"è±†ç“£å°ç»„ {GROUP_ID} çƒ­é—¨å¸–å­",
        "elite": f"è±†ç“£å°ç»„ {GROUP_ID} ç²¾åæ¨è"
    }
    
    descriptions = {
        "new": "æŒ‰å‘å¸ƒæ—¶é—´æ’åºçš„æœ€æ–°å°ç»„è®¨è®º",
        "hot": "æŒ‰å›å¤æ•°æ’åºçš„çƒ­é—¨å°ç»„è®¨è®º",
        "elite": "è±†ç“£å°ç»„ç²¾åå¸–ç²¾é€‰"
    }
    
    # åˆ›å»ºRSSæ ¹å…ƒç´ 
    rss = ET.Element('rss', version='2.0')
    
    # åˆ›å»ºé¢‘é“
    channel = ET.SubElement(rss, 'channel')
    ET.SubElement(channel, 'title').text = titles[feed_type]
    ET.SubElement(channel, 'link').text = f"https://www.douban.com/group/{GROUP_ID}"
    ET.SubElement(channel, 'description').text = descriptions[feed_type]
    ET.SubElement(channel, 'language').text = "zh-cn"
    
    # æ·»åŠ å¸–å­é¡¹ï¼ˆå¸¦æ’åï¼‰
    for index, post in enumerate(posts):
        # æ ¹æ®ç±»å‹æ·»åŠ ä¸åŒçš„æ ‡é¢˜å‰ç¼€
        if feed_type == "elite":
            title_prefix = f"ğŸ”¥ ç²¾å #{index+1}: "
        elif feed_type == "hot":
            title_prefix = f"ğŸ”¥ çƒ­é—¨ #{index+1}: "
        else:
            title_prefix = f"ğŸ†• æœ€æ–° #{index+1}: "
        
        # æ·»åŠ åˆ†ç±»æ ‡ç­¾
        if post.get("category"):
            title_prefix = post["category"] + " " + title_prefix
        
        item = ET.SubElement(channel, 'item')
        ET.SubElement(item, 'title').text = title_prefix + post['title']
        ET.SubElement(item, 'link').text = post['link']
        ET.SubElement(item, 'pubDate').text = post['pubDate'].strftime('%a, %d %b %Y %H:%M:%S GMT')
        
        # æ„å»ºè¯¦ç»†æè¿°
        description = f"å‘å¸ƒäº: {post['raw_time']}"
        if post.get("reply_count", 0) > 0:
            description += f" | å›å¤æ•°: {post['reply_count']}"
        if post.get("author"):
            description += f" | ä½œè€…: {post['author']}"
        if post.get("is_elite"):
            description += " | ğŸ”¥ç²¾åå¸–"
        
        ET.SubElement(item, 'description').text = description
    
    # ç”ŸæˆXMLå­—ç¬¦ä¸²
    return '<?xml version="1.0" encoding="utf-8"?>\n' + ET.tostring(rss, encoding='unicode')

# ===== ä¸»ç¨‹åº =====
if __name__ == "__main__":
    print("=" * 60)
    print(f"ğŸš€ è±†ç“£å°ç»„ RSS ç”Ÿæˆå™¨å¯åŠ¨ - å°ç»„ID: {GROUP_ID}")
    print(f"â±ï¸ å¼€å§‹æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    try:
        # æŠ“å–è®¨è®ºåŒºå¸–å­
        discussion_posts = fetch_discussion_posts()
        
        # æŠ“å–ç²¾ååŒºå¸–å­
        elite_posts = fetch_elite_posts()
        
        # ç”Ÿæˆä¸‰ç§æ’åºçš„å¸–å­åˆ—è¡¨
        new_posts = sorted(discussion_posts, key=lambda x: x["pubDate"], reverse=True)[:50]  # æœ€æ–°50æ¡
        hot_posts = sorted(discussion_posts, key=lambda x: x["reply_count"], reverse=True)[:50]  # æœ€çƒ­50æ¡
        
        # ç”Ÿæˆä¸‰ä¸ªRSSæº
        rss_new = generate_rss(new_posts, "new")
        rss_hot = generate_rss(hot_posts, "hot")
        rss_elite = generate_rss(elite_posts, "elite")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"douban_{GROUP_ID}_new.xml", "w", encoding="utf-8") as f:
            f.write(rss_new)
        
        with open(f"douban_{GROUP_ID}_hot.xml", "w", encoding="utf-8") as f:
            f.write(rss_hot)
        
        with open(f"douban_{GROUP_ID}_elite.xml", "w", encoding="utf-8") as f:
            f.write(rss_elite)
        
        print("\nğŸ‰ RSSæ–‡ä»¶å·²ç”Ÿæˆ:")
        print(f"ğŸ“° æœ€æ–°å¸–: douban_{GROUP_ID}_new.xml ({len(new_posts)}æ¡)")
        print(f"ğŸ”¥ çƒ­é—¨å¸–: douban_{GROUP_ID}_hot.xml ({len(hot_posts)}æ¡)")
        print(f"ğŸŒŸ ç²¾åå¸–: douban_{GROUP_ID}_elite.xml ({len(elite_posts)}æ¡)")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        if new_posts:
            print("\nğŸ“¢ æœ€æ–°å¸–ç¤ºä¾‹:")
            for i, post in enumerate(new_posts[:3]):
                print(f"  {i+1}. [{post['raw_time']}] {post['category']}{post['title']}")
        
        if hot_posts:
            print("\nğŸ”¥ çƒ­é—¨å¸–ç¤ºä¾‹:")
            for i, post in enumerate(hot_posts[:3]):
                print(f"  {i+1}. [{post['reply_count']}å›å¤] {post['category']}{post['title']}")
        
        if elite_posts:
            print("\nğŸŒŸ ç²¾åå¸–ç¤ºä¾‹:")
            for i, post in enumerate(elite_posts[:3]):
                print(f"  {i+1}. [{post['author']}] {post['category']}{post['title']}")
        
    except Exception as e:
        print(f"\nğŸ”¥ ä¸¥é‡é”™è¯¯: {str(e)}")
        print("ğŸ†˜ åˆ›å»ºç©ºRSSæ–‡ä»¶...")
        
        # åˆ›å»ºä¸‰ä¸ªç©ºæ–‡ä»¶é˜²æ­¢å·¥ä½œæµå¤±è´¥
        for feed_type in ["new", "hot", "elite"]:
            filename = f"douban_{GROUP_ID}_{feed_type}.xml"
            with open(filename, "w", encoding="utf-8") as f:
                f.write('<?xml version="1.0" encoding="utf-8"?>\n')
                f.write('<rss version="2.0">\n')
                f.write('<channel>\n')
                f.write(f'<title>è±†ç“£å°ç»„ {GROUP_ID} {"æœ€æ–°" if feed_type=="new" else "çƒ­é—¨" if feed_type=="hot" else "ç²¾å"}å¸–</title>\n')
                f.write('<description>æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—</description>\n')
                f.write('</channel>\n')
                f.write('</rss>')
            print(f"ğŸ“ å·²åˆ›å»ºç©ºæ–‡ä»¶: {filename}")
    
    print("\nğŸ ä»»åŠ¡å®Œæˆ!")
